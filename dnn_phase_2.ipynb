{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors:\n",
    "- Jakub Janicki\n",
    "- Kuba Kondracki\n",
    "- Hubert Twardowski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janic\\anaconda3\\envs\\dnn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from nlp import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALBERT_MODEL = 'albert-base-v2'    # https://huggingface.co/albert/albert-base-v2\n",
    "tokenizer = transformers.AlbertTokenizer.from_pretrained(ALBERT_MODEL)\n",
    "    \n",
    "def encode_batch(batch):\n",
    "    return tokenizer(batch['text_combined'], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# precompute the train and test datasets\n",
    "if os.path.exists('train_dataset.pkl') and os.path.exists('test_dataset.pkl'):\n",
    "    with open('train_dataset.pkl', 'rb') as f:\n",
    "        train_dataset = pickle.load(f)\n",
    "    with open('test_dataset.pkl', 'rb') as f:\n",
    "        test_dataset = pickle.load(f)\n",
    "else:\n",
    "    df = pd.read_csv('phishing_email.csv')[['text_combined', 'label']].dropna()\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    data_dct = dataset.train_test_split(train_size=0.7)\n",
    "    train_dataset = data_dct['train'].map(encode_batch, batched=True)\n",
    "    test_dataset = data_dct['test'].map(encode_batch, batched=True)\n",
    "    with open('train_dataset.pkl', 'wb') as f:\n",
    "        pickle.dump(train_dataset, f)\n",
    "    with open('test_dataset.pkl', 'wb') as f:\n",
    "        pickle.dump(test_dataset, f)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"SAFE\", 1: \"PHISHING\"}\n",
    "label2id = {\"PHISHING\": 1, \"SAFE\": 0}\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    f\"albert/{ALBERT_MODEL}\", num_labels=2, id2label=id2label, label2id=label2id, attn_implementation=\"sdpa\")\n",
    "# SDPA also from ALBERT usage guide\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    auto_find_batch_size=True,\n",
    "    dataloader_num_workers=4,\n",
    "     # this model is too large to tune hyperparameters. \n",
    "     # lr of 1e-5 has always worked fine with models we've trained previously, with some wd like 1e-8 or 1e-10 preventing explosions.\n",
    "     # instead of hparam tuning, we decided to use lr scheduler to diminish the impact of incorrect lr.\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr\": 1e-8},\n",
    "    weight_decay=1e-8,\n",
    "    num_train_epochs=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500, # checkpoint every 500 steps\n",
    "    bf16=True, # bfloat16 since im getting {\"grad_norm\": nan} with float16 and i cant quite run float32\n",
    "    report_to=\"tensorboard\", # setup tensorboard\n",
    "    max_steps=1000, # limit the number of steps to 1000\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 65988\n",
      "Test dataset size: 16498\n",
      "Training label distribution: 0.521534218342729\n",
      "Test label distribution: 0.5137592435446721\n",
      "First train sample: {'label': tensor(0), 'input_ids': tensor([    2,  2247,  1266,   289,   530,  4696,   315,   169,   883, 21156,\n",
      "          228,   286,   883,  7681, 21156,   315,   169,  1067,   296,   303,\n",
      "         1100,   148, 11597,  7912,   315,   169,  2094,   883,  4247,   169,\n",
      "          883,   289,   913,  2824,   193,  1015,  3062,   138,  1100,   148,\n",
      "        11597,  7912,  1364,   265,   883,  1466,   868,    83,  1466,   651,\n",
      "         1100,   148,   289,  3553,  1466, 12238, 21156,  2945,  7681, 21156,\n",
      "         1100,   148,  7912,    43,  1148,  9021,   442,  6305,  1696, 21156,\n",
      "          883,   169,    87,   910,   193,   868,  2670, 21156,  3081,   955,\n",
      "          529,  4466,  2383,  1139,   341, 14403,  5457,   609,  1131,  2247,\n",
      "         1266,   530,   134,  1832,    28,    18,   254,    34,    62,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "Model architecture: AlbertForSequenceClassification(\n",
      "  (albert): AlbertModel(\n",
      "    (embeddings): AlbertEmbeddings(\n",
      "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (encoder): AlbertTransformer(\n",
      "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
      "      (albert_layer_groups): ModuleList(\n",
      "        (0): AlbertLayerGroup(\n",
      "          (albert_layers): ModuleList(\n",
      "            (0): AlbertLayer(\n",
      "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): AlbertSdpaAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (attention_dropout): Dropout(p=0, inplace=False)\n",
      "                (output_dropout): Dropout(p=0, inplace=False)\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              )\n",
      "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (pooler_activation): Tanh()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Training label distribution: {sum(train_dataset['label']) / len(train_dataset['label'])}\")\n",
    "print(f\"Test label distribution: {sum(test_dataset['label']) / len(test_dataset['label'])}\")\n",
    "\n",
    "print(f\"First train sample: {train_dataset[0]}\")\n",
    "print(f\"Model architecture: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs will be stored in: ./logs\n"
     ]
    }
   ],
   "source": [
    "print(f\"TensorBoard logs will be stored in: {training_args.logging_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [02:47<00:00, 12.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline metrics: {'eval_loss': 0.715407133102417, 'eval_model_preparation_time': 0.0, 'eval_accuracy': 0.51709298096739, 'eval_runtime': 174.6065, 'eval_samples_per_second': 94.487, 'eval_steps_per_second': 11.815}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_metrics = trainer.evaluate()\n",
    "print(f\"Baseline metrics: {baseline_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(0),\n",
       " 'input_ids': tensor([    2,  2247,  1266,   289,   530,  4696,   315,   169,   883, 21156,\n",
       "           228,   286,   883,  7681, 21156,   315,   169,  1067,   296,   303,\n",
       "          1100,   148, 11597,  7912,   315,   169,  2094,   883,  4247,   169,\n",
       "           883,   289,   913,  2824,   193,  1015,  3062,   138,  1100,   148,\n",
       "         11597,  7912,  1364,   265,   883,  1466,   868,    83,  1466,   651,\n",
       "          1100,   148,   289,  3553,  1466, 12238, 21156,  2945,  7681, 21156,\n",
       "          1100,   148,  7912,    43,  1148,  9021,   442,  6305,  1696, 21156,\n",
       "           883,   169,    87,   910,   193,   868,  2670, 21156,  3081,   955,\n",
       "           529,  4466,  2383,  1139,   341, 14403,  5457,   609,  1131,  2247,\n",
       "          1266,   530,   134,  1832,    28,    18,   254,    34,    62,     3,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for correct padding\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/albert usage tip:\n",
    "# ALBERT is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/1000 [00:32<03:38,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4846, 'grad_norm': 64.0195083618164, 'learning_rate': 9.755527298894292e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|█         | 100/1000 [03:28<03:38,  4.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2660793662071228, 'eval_accuracy': 0.9004727845799491, 'eval_runtime': 176.0971, 'eval_samples_per_second': 93.687, 'eval_steps_per_second': 11.715, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 200/1000 [03:52<03:15,  4.10it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3478, 'grad_norm': 19.467729568481445, 'learning_rate': 9.046039886902864e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|██        | 200/1000 [06:53<03:15,  4.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2388971745967865, 'eval_accuracy': 0.9253242817311189, 'eval_runtime': 180.2094, 'eval_samples_per_second': 91.549, 'eval_steps_per_second': 11.448, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 300/1000 [07:16<02:43,  4.28it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3681, 'grad_norm': 77.08357238769531, 'learning_rate': 7.940987335200904e-06, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 30%|███       | 300/1000 [10:11<02:43,  4.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30863192677497864, 'eval_accuracy': 0.9216874772699721, 'eval_runtime': 174.8873, 'eval_samples_per_second': 94.335, 'eval_steps_per_second': 11.796, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 400/1000 [10:36<02:25,  4.12it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2629, 'grad_norm': 85.00262451171875, 'learning_rate': 6.548539886902863e-06, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 40%|████      | 400/1000 [13:31<02:25,  4.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24285057187080383, 'eval_accuracy': 0.9478724693902291, 'eval_runtime': 175.3395, 'eval_samples_per_second': 94.092, 'eval_steps_per_second': 11.766, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 500/1000 [13:55<02:02,  4.09it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1968, 'grad_norm': 0.056783415377140045, 'learning_rate': 5.0049999999999995e-06, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 50%|█████     | 500/1000 [16:51<02:02,  4.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25608712434768677, 'eval_accuracy': 0.9495090313977452, 'eval_runtime': 176.0272, 'eval_samples_per_second': 93.724, 'eval_steps_per_second': 11.72, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 600/1000 [17:16<01:35,  4.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2929, 'grad_norm': 158.9840850830078, 'learning_rate': 3.461460113097138e-06, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 60%|██████    | 600/1000 [20:09<01:35,  4.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4986262023448944, 'eval_accuracy': 0.8962904594496303, 'eval_runtime': 173.4406, 'eval_samples_per_second': 95.122, 'eval_steps_per_second': 11.895, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 700/1000 [20:33<01:10,  4.26it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2633, 'grad_norm': 0.0711698830127716, 'learning_rate': 2.0690126647990974e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 70%|███████   | 700/1000 [23:26<01:10,  4.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1919512003660202, 'eval_accuracy': 0.9565401866892956, 'eval_runtime': 173.382, 'eval_samples_per_second': 95.154, 'eval_steps_per_second': 11.899, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 800/1000 [23:50<00:46,  4.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1846, 'grad_norm': 110.15643310546875, 'learning_rate': 9.63960113097138e-07, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 80%|████████  | 800/1000 [26:46<00:46,  4.30it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1652226448059082, 'eval_accuracy': 0.9618741665656443, 'eval_runtime': 175.8193, 'eval_samples_per_second': 93.835, 'eval_steps_per_second': 11.734, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 900/1000 [27:10<00:24,  4.10it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1885, 'grad_norm': 0.7435556650161743, 'learning_rate': 2.544727011057081e-07, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 90%|█████████ | 900/1000 [30:05<00:24,  4.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16509948670864105, 'eval_accuracy': 0.9628439810886168, 'eval_runtime': 174.4278, 'eval_samples_per_second': 94.584, 'eval_steps_per_second': 11.827, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [30:29<00:00,  4.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1602, 'grad_norm': 0.3658551275730133, 'learning_rate': 1e-08, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1000/1000 [33:24<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16470371186733246, 'eval_accuracy': 0.9627227542732453, 'eval_runtime': 174.9576, 'eval_samples_per_second': 94.297, 'eval_steps_per_second': 11.791, 'epoch': 0.12}\n",
      "{'train_runtime': 2004.5988, 'train_samples_per_second': 3.991, 'train_steps_per_second': 0.499, 'train_loss': 0.27497378349304197, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [02:47<00:00, 12.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model(\"albert-base-v2-phishing-emails\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [02:48<00:00, 12.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16470371186733246, 'eval_accuracy': 0.9627227542732453, 'eval_runtime': 175.2363, 'eval_samples_per_second': 94.147, 'eval_steps_per_second': 11.773, 'epoch': 0.12122681537156019}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
